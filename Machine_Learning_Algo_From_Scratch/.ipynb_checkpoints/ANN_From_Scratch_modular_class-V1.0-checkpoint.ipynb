{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN Implementation From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Attr_A</th>\n",
       "      <th>Attr_B</th>\n",
       "      <th>Attr_C</th>\n",
       "      <th>Attr_D</th>\n",
       "      <th>Result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0.399123</td>\n",
       "      <td>0.587303</td>\n",
       "      <td>0.535581</td>\n",
       "      <td>0.858321</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>692</th>\n",
       "      <td>0.794640</td>\n",
       "      <td>0.932635</td>\n",
       "      <td>0.058539</td>\n",
       "      <td>0.382244</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>838</th>\n",
       "      <td>0.340451</td>\n",
       "      <td>0.633837</td>\n",
       "      <td>0.184630</td>\n",
       "      <td>0.530393</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1289</th>\n",
       "      <td>0.406197</td>\n",
       "      <td>0.435848</td>\n",
       "      <td>0.223238</td>\n",
       "      <td>0.759791</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1349</th>\n",
       "      <td>0.393527</td>\n",
       "      <td>0.434688</td>\n",
       "      <td>0.301303</td>\n",
       "      <td>0.859261</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Attr_A    Attr_B    Attr_C    Attr_D  Result\n",
       "123   0.399123  0.587303  0.535581  0.858321       0\n",
       "692   0.794640  0.932635  0.058539  0.382244       0\n",
       "838   0.340451  0.633837  0.184630  0.530393       1\n",
       "1289  0.406197  0.435848  0.223238  0.759791       1\n",
       "1349  0.393527  0.434688  0.301303  0.859261       1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset='banknote'\n",
    "df=pd.read_csv('Dataset/banknote.csv').sample(frac=1.0)\n",
    "N=len(df)\n",
    "\n",
    "df=df.fillna(0)\n",
    "feats=df.columns.values[:-1]\n",
    "target=df.columns.values[-1]\n",
    "scaler=MinMaxScaler()\n",
    "scaler.fit(df[feats])\n",
    "df[feats]=scaler.transform(df[feats])\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train=int(N*0.85)\n",
    "_train_df=df.loc[:n_train]\n",
    "_val_df=df.loc[n_train:]\n",
    "\n",
    "_train_x=np.array(_train_df[feats],dtype=np.float64)\n",
    "_train_y=np.array(_train_df[target],dtype=np.float64)\n",
    "_val_x=np.array(_val_df[feats],dtype=np.float64)\n",
    "_val_y=np.array(_val_df[target],dtype=np.float64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network :\n",
    "<ul style=\"list-style-type:none;\">  \n",
    "   <li><b> Forward Propagation : </b></li>\n",
    "          <ul>\n",
    "          <li> Output of a neuron : Z = W*X + B , Mentioned as Neuron Output </li>\n",
    "            <ul style=\"list-style-type:circle;\">\n",
    "            <li> W(a,b) a for current layer, b for previous layer </li>\n",
    "            <li> Layer1:Z11 = Node1:W11 * X1 + Node2:W12 * X2 + Layer2:Bias </li>\n",
    "            <li> Layer1:Z12 = Node1:W11 * X1 + Node2:W12 * X2 + Layer2:Bias </li>\n",
    "            <li> Layer1:Z13 = Node1:W11 * X1 + Node2:W12 * X2 + Layer2:Bias </li>\n",
    "            </ul>\n",
    "          <li>(For detail better under understanding mutual weight indexing W(a,b) see appendix on Nodes)</li>\n",
    "          </ul>\n",
    "<li><b>  Calulating gradients : </b></li>\n",
    "          <ul>          \n",
    "          <li> Calculatin dE/dW </li>\n",
    "             <ul>\n",
    "             <li> dE/dW = dE/dA * dA/dZ * dZ/dw \n",
    "             </ul>\n",
    "          <li> Calculatin dE/dA </li>\n",
    "                <ul>\n",
    "                <li>dE/dA : For Last Layer, if loss function is mean square error, then E = 1/2*(Y - A)^2 so , dE/dA = (A-Y) </li>\n",
    "                <li> dE/dA : For Other Layers dE/dA will be inherited from the each of the node of next layer acccording\n",
    "                to mutual weights. </li>\n",
    "                <li> Layer2:Error1 = layer3:Error1 * Layer3:W11 + Layer3:Error2 * Layer3:W21 </li>\n",
    "                <li> Mathmetically Implmented as Error_l2 = Weights_l3.Transpose() * Error_l3 </li>\n",
    "                </ul>\n",
    "          <li>Calculating dA/dZ (dA/dZ : grad of Activation Output wrt each Neuron output  also mentioned as delta here) </li>\n",
    "                <ul>\n",
    "                <li> For last layer Derivative of sigmoid function in : dA/dZ = A*(1-A)\n",
    "                <li> For other layers : dA/dZ = 1\n",
    "                </ul>\n",
    "          <li> Calculating dZ/dW (dZ/dW : grad of Neuron Output wrt each weights ) </li>\n",
    "                 <ul>\n",
    "                 <li> As Z = W*X + b so dZ/dW = X , for first layer, X is the input  </li>\n",
    "                 <li> For other layers As Z = W*A + b so dZ/dW = A, here A is the activation value from previous layer. </li> \n",
    "                 </ul>\n",
    "          </ul>\n",
    "<li><b>  Updating Weights: </b></li>\n",
    "    <ul>\n",
    "    <li> Updating each weights W = W - lr*dE/dW </li>\n",
    "    <li> Here lr is the learning rate. </li>\n",
    "    </ul>  \n",
    "<li><b> Note : </b></li> In the flowchart Gradient calcultion & Back Propagation is shown in back propagation.\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sigmoid():\n",
    "    def apply(x):\n",
    "       y=np.exp(-1*x)+1\n",
    "       y=1/y\n",
    "       #print(y)\n",
    "       return y\n",
    "    def deriv(x):\n",
    "        return x*(1-x)\n",
    "    \n",
    "class none():  \n",
    "    def apply(x):\n",
    "        return x\n",
    "    def deriv(x):\n",
    "        return 1\n",
    "    \n",
    "class relu():\n",
    "    def apply(x):\n",
    "        return x\n",
    "    def deriv(x):\n",
    "        deriv=x.copy()\n",
    "        deriv[x<=0]=0\n",
    "        deriv[x>0]=1\n",
    "\n",
    "        return deriv\n",
    "\n",
    "class Layer():\n",
    "    \n",
    "    def __init__(self,name='no_name',pnd=1,nd=0,act_func=none):\n",
    "        self.name=name\n",
    "        self.value=np.zeros((nd),dtype=np.float64)\n",
    "        self.activation=np.zeros((nd),dtype=np.float64)\n",
    "        #self.weights=np.random.random(size=(nd,pnd))*2.0-1.0\n",
    "        self.weights=np.random.random(size=(pnd,nd))*2.0-1.0\n",
    "        self.delta=np.zeros((nd,pnd),dtype=np.float64)\n",
    "        self.deriv=np.zeros((nd,pnd),dtype=np.float64)\n",
    "        self.error_deriv=np.zeros((nd),dtype=np.float64)\n",
    "        self.act_func  = act_func\n",
    "        \n",
    "    def __call__(self,_input):\n",
    "         if self.name=='Input':\n",
    "                self.activation=_input\n",
    "         else:\n",
    "             #self.value=np.matmul(self.weights,_input)\n",
    "             self.value=np.matmul(_input,self.weights)\n",
    "             self.activation=self.act_func.apply(self.value)\n",
    "         return self.activation\n",
    "        \n",
    "    def printl(self):\n",
    "        print(self.name,end=' ')\n",
    "        print('Node',self.weights.shape[1])\n",
    "    \n",
    "    @property\n",
    "    def act_deriv(self):\n",
    "        return self.act_func.deriv(self.activation)\n",
    "\n",
    "\n",
    "class Network(list):\n",
    "    \n",
    "    def add(self,_layer):\n",
    "        _layer.printl()\n",
    "        self.append(_layer) \n",
    "        \n",
    "    def for_prop(self,_input):\n",
    "        for j in range(len(self)):\n",
    "            Layer=self[j]\n",
    "            out=Layer(_input)\n",
    "            _input=out\n",
    "        return out\n",
    "\n",
    "    def back_prop(self,expected):\n",
    "        for j in reversed(range(1,len(self))):\n",
    "            Layer=self[j]\n",
    "            Prev_Layer=self[j-1]\n",
    "            \n",
    "            if Layer.name=='Output':\n",
    "                error = (expected - Layer.activation)**2\n",
    "                Layer.error_deriv=2*(Layer.activation-expected.reshape(Layer.activation.shape[0],-1)) # dE/dA\n",
    "\n",
    "            else :\n",
    "                Next_Layer=self[j+1]\n",
    "                weights=Next_Layer.weights.T\n",
    "                Layer.error_deriv=np.matmul(Next_Layer.error,weights)\n",
    "\n",
    "            Layer.deriv=Layer.act_deriv*Layer.error_deriv #dE/dA * dA/dZ\n",
    "            batch_size=Layer.activation.shape[0]\n",
    "            Layer.delta=np.zeros_like(Layer.weights)\n",
    "            for i in range(batch_size):                                        # For each sample in batch\n",
    "                for k in range(Layer.deriv.shape[1]):\n",
    "                    Layer.delta[:,k] += Prev_Layer.activation[i,:]*Layer.deriv[i,k] #dZ/dW * (dE/dA * dA/dZ)\n",
    "\n",
    "            Layer.delta /= batch_size\n",
    "        return err\n",
    "\n",
    "    def weight_update(self,alpha=0.01):\n",
    "        for j in range(1,len(self)):\n",
    "            Layer=self[j]\n",
    "            Layer.weights-=alpha*Layer.delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Node 4\n",
      "Output Node 1\n"
     ]
    }
   ],
   "source": [
    "error_log=np.zeros((100),np.float64)\n",
    "epoc_val=np.zeros((100),np.float64)\n",
    "     \n",
    "\n",
    "net=Network()\n",
    "_layer=Layer('Input',0,len(feats))\n",
    "net.add(_layer)\n",
    "#_layer=Layer('Hidden_one',len(feats),3,act_func=relu)\n",
    "#net.add(_layer)\n",
    "_layer=Layer('Output',len(feats),1,act_func=sigmoid)\n",
    "net.add(_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████████████████████████████████████████▌                                         | 49/99 [00:05<00:05,  8.99it/s]"
     ]
    }
   ],
   "source": [
    "#net=network()\n",
    "from tqdm import tqdm\n",
    "_no_epochs=100\n",
    "_epoch=np.zeros((_no_epochs),np.int64)\n",
    "_error=np.zeros((_no_epochs,2),np.float64)\n",
    "batch_size=32\n",
    "for epoch in tqdm(range(1,_no_epochs)):\n",
    "    err=0\n",
    "    for i in range(len(_train_x)//batch_size):\n",
    "        \n",
    "        _x_batch=_train_x[i*batch_size:(i+1)*batch_size]\n",
    "        _y_batch=_train_y[i*batch_size:(i+1)*batch_size]\n",
    "        \n",
    "        net.for_prop(_x_batch)\n",
    "        net.back_prop(_y_batch)\n",
    "        net.weight_update()\n",
    "        #net=updateWeights(net,X[i],.05)\n",
    "        #_,pred_tr_Y=for_prop(net,_X_batch.T)\n",
    "        _pred_y_batch=net.for_prop(_x_batch)\n",
    "        train_error=np.array((_y_batch-_pred_y_batch)**2).mean() #for j in range(len(Y[i]))\n",
    "        \n",
    "        \n",
    "        _x_batch_val=_val_x#[rind:rind+batch_size]\n",
    "        _y_batch_val=_val_y#[rind:rind+batch_size]\n",
    "        pred_val_y=net.for_prop(_x_batch_val)\n",
    "        val_error=np.array((_y_batch_val-pred_val_y)**2).mean() #for j in range(len(Y[i]))\n",
    "        #print(epock)\n",
    "        #print(e)\n",
    "        _epoch[epoch-1]=epoch\n",
    "        _error[epoch-1,0]=train_error\n",
    "        _error[epoch-1,1]=val_error\n",
    "_epoch[epoch]=epoch\n",
    "_error[epoch,0]=train_error\n",
    "_error[epoch,1]=val_error\n",
    "#print(sigmoid_deriv(.93))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(_y_batch_val-pred_val_y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(_epoch[0:],_error[0:])\n",
    "plt.savefig(dataset+' ann_loss')\n",
    "plt.title('epoch vs loss(MSE)')\n",
    "plt.legend(['train loss','validation loss'])\n",
    "plt.show()\n",
    "#_,v=for_prop(net,X.T)\n",
    "#print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret=net.for_prop(_val_x)\n",
    "ret=ret.reshape(-1).copy()\n",
    "ret[ret>=0.4]=1.0\n",
    "ret[ret<0.4]=0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('accuracy',np.sum(ret==_val_y)/len(_val_y))\n",
    "result_log=pd.DataFrame({})\n",
    "result_log['true']=_val_y\n",
    "result_log['pred']=ret\n",
    "result_log['right']=np.array((ret==_val_y),dtype=np.uint8)\n",
    "result_log['wrong']=np.array((ret!=_val_y),dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "#plt.plot(result_log[['right','wrong']])\n",
    "\n",
    "fig=plt.figure(figsize=(15,5))\n",
    "plt.plot(result_log.loc[:100,'true'],'r--',alpha=0.6)\n",
    "plt.plot(result_log.loc[:100,'pred'],'bx',alpha=0.6)\n",
    "plt.plot(result_log.loc[:100,'right'][result_log.loc[:100,'right']==1]+0.05,'go',alpha=0.4)\n",
    "plt.plot(result_log.loc[:100,'right'][result_log.loc[:100,'right']==0]+1.05,'ro',alpha=0.4)\n",
    "#plt.plot(result_log.loc[:100,'right'],'go',alpha=0.3)\n",
    "plt.legend(['true','pred','right','wrong'])\n",
    "plt.title(dataset+' dataset with nn prediction correctness plot')# \\n right : 1 \\n wrong : 0')\n",
    "plt.savefig(dataset+'_ann_pred.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from copy import deepcopy\n",
    "#plt.plot(result_log[['right','wrong']])\n",
    "dataset='banknote'\n",
    "\n",
    "labels=[0.0,1.0]\n",
    "result_grps=result_log.groupby('pred')\n",
    "fig,axes=plt.subplots(nrows=len(labels),figsize=(15,8))\n",
    "for i,label in enumerate(labels):\n",
    "    \n",
    "    #_ndf=result_grps.get_group(label)\n",
    "    ndf= deepcopy(result_log.loc[:100])\n",
    "    ndf['tp']=np.array(((ndf.true==label) & (ndf.pred==label)),dtype=np.uint8)\n",
    "    ndf['fn']=np.array(((ndf.true==label) & (ndf.pred!=label)),dtype=np.uint8)\n",
    "    ndf['fp']=np.array(((ndf.true!=label) & (ndf.pred==label)),dtype=np.uint8)\n",
    "\n",
    "    ndf['final']=np.array(['None']*len(ndf),dtype=np.object)\n",
    "    #ndf.final[ndf.tp==1]='True_Pos'\n",
    "    #ndf.final[ndf.tn==1]='True_Neg'\n",
    "    #ndf.final[ndf.fp==1]='False_Pos'\n",
    "\n",
    "    #plt.plot(ndf.index,ndf.final)\n",
    "    axes[i].plot(ndf.tp[ndf.tp==1]*3,'go',alpha=0.6)\n",
    "    axes[i].plot(ndf.fn[ndf.fn==1]*2,'ro',alpha=0.6)\n",
    "    axes[i].plot(ndf.fp[ndf.fp==1]*1,'bo',alpha=0.3)\n",
    "    #plt.plot(result_log.loc[:100,'right'][result_log.loc[:100,'right']==0]+1.05,'ro',alpha=0.3)\n",
    "    #plt.plot(result_log.loc[:100,'right'],'go',alpha=0.3)\n",
    "    axes[i].legend(['true pos','true neg','false pos'])\n",
    "    axes[i].set_title('ANN tp fn fp plot label : '+str(label)+' dataset : '+dataset)# \\n right : 1 \\n wrong : 0')\n",
    "plt.savefig(dataset+'tp_fp.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total=len(result_log)\n",
    "acc=result_log.right.sum()#,len(result_log)\n",
    "accuracy=acc/total\n",
    "labels=result_log['true'].unique()\n",
    "ldf=pd.DataFrame(data={},index=[' '])\n",
    "precisions={}\n",
    "recalls={}\n",
    "for label in labels:\n",
    "    true_ind=result_log['true']==label\n",
    "    false_ind=result_log['true']!=label\n",
    "    Yes=result_log.loc[true_ind]\n",
    "    No=result_log.loc[false_ind]\n",
    "    tp=Yes.right.sum()\n",
    "    tn=No.right.sum()\n",
    "    fn=Yes.wrong.sum() #wrong after negative prediction means positive\n",
    "    fp=No.wrong.sum() #wrong after positive prediction means negative\n",
    "    recall=tp/(tp+fn)\n",
    "    precision=tp/(tp+fp)\n",
    "    precisions[label]=precision\n",
    "    recalls[label]=recall\n",
    "    print('----------------------')\n",
    "    print('label',label)\n",
    "    print('precision',precision)\n",
    "    print('recall',recall)\n",
    "    print('----------------------')\n",
    "print('Over All Accuracy : ',accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "d = {'col1': [1, 2], 'col2': [3, 4]}\n",
    "df = pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame({},index=[' '])\n",
    "#fig=plt.figure(figsize=(10,10))\n",
    "for label in result_log['true'].unique():\n",
    "    df['class-'+str(label)+'-precision']=[precisions[label],]\n",
    "    df['calss-'+str(label)+'-recall']=[recalls[label],]\n",
    "df['accuracy']=accuracy \n",
    "df.plot.bar()\n",
    "plt.xlim(-.3,.8)\n",
    "plt.title('test result for ' +dataset+ ' data \\n '+'train '+str(len(_train_df))+' samples test '+str(len(result_log))+' samples')\n",
    "plt.xlabel('summary of precision, recall, accuracy for iris dataset')\n",
    "plt.savefig(dataset+'_ann.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Nodes : </h4>\n",
    "<div style=\"background-color:#F8F9F9\">\n",
    "    <ul style=\"list-style-type:square;\">  \n",
    "    <li> Every Node/Neuron is considered to have weights for each of previous layers node.</li>\n",
    "    </ul>\n",
    "         <br>Leyer 1 ->2 nodes \n",
    "         <br>Layer 2 ->3 Nodes\n",
    "         <br>So, Layer 2's each of the 3 Nodes will have two weights for Layer 1's each of the 2 nodes.\n",
    "         <br>they are -\n",
    "         <br>Layer 2 - Node 1: W11 , W12\n",
    "         <br>Layer 2 - Node 2: W21 , W22\n",
    "         <br>Layer 2 - Node 3: W31 , W32\n",
    "         <br>Value of Layer 2:Node1 (Z21) = X1 * Layer2:W11 + X2 * Layer2:W12\n",
    "         <br>Mutual Weights: Layer:W(a,b) : \n",
    "             <br>a is the node we are calculating for (current layer)\n",
    "             <br>b is the contributing node (from previous layer)\n",
    "             <br>i.e \n",
    "              <br>Layer2:W12 --> Layer 2's 1st node's weight for previous layers(Layer 1) 2nd node\n",
    "              <br>Layer2:W13 --> Layer 2's 1st node's weight for previous layers(Layer 1) 3rd node\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Debug Codes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.ones((21),np.int8)\n",
    "for i in range(0,21):\n",
    "    x[i]=i-10\n",
    "y=np.exp(-1*x)\n",
    "y=1+y\n",
    "y=1/y\n",
    "p=(x+10)/20\n",
    "z=np.log(p)\n",
    "print(x)\n",
    "plt.plot(x,y)\n",
    "plt.show()\n",
    "plt.plot(p,z)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.argmax([.2,.3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
