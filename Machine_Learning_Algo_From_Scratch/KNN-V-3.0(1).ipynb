{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.dom import minidom\n",
    "import re\n",
    "import pandas as pd\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "ps = PorterStemmer()\n",
    "sys.__stdout__=sys.stdout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing steps\n",
    "\n",
    "1. Stem regular verbs\n",
    "2. Stem irregular verbs\n",
    "3. Removing minor words\n",
    "4. Removing tags and garbage charecters with rtegular expression.\n",
    "\n",
    "*All of these is implemented with raw python (without nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_regular_verb(word):\n",
    "    if len(word)<=3:\n",
    "        return word\n",
    "    if word[-2:]=='es':\n",
    "        if word[:-2] in verbs:\n",
    "            word=word[:-2]\n",
    "\n",
    "    elif word[-1:]=='s':\n",
    "        if word[:-1] in verbs:\n",
    "            word=word[:-1]\n",
    "            \n",
    "    elif word[-3:]=='ing':\n",
    "        if word[:-3] in verbs:\n",
    "            word=word[:-3]\n",
    "        if word[:-4] in verbs:\n",
    "            word=word[:-4]\n",
    "        else:\n",
    "            word=word[:-3]\n",
    "    \n",
    "    elif word[-2:]=='ed':\n",
    "        if word[:-2] in verbs:\n",
    "            word=word[:-2]\n",
    "        elif word[:-1] in verbs:\n",
    "            word=word[:-1]\n",
    "    #word=stem_irregular(word)\n",
    "    return word\n",
    "def stem_simple(word):\n",
    "    if word[-2:]=='es':\n",
    "            word=word[:-2]   \n",
    "    elif word[-1:]=='s':\n",
    "            word=word[:-1]\n",
    "    elif word[-3:]=='ing':\n",
    "            word=word[:-3]\n",
    "    \n",
    "    elif word[-2:]=='ed':\n",
    "            word=word[:-2]\n",
    "    return word\n",
    "\n",
    "def stem_irregular_verb(word):\n",
    "        if word in past:\n",
    "            ind=past.index(word)\n",
    "            word=verb_root[ind]\n",
    "        elif word in past_pp:\n",
    "            ind=past_pp.index(word)\n",
    "            word=verb_root[ind]\n",
    "        return word\n",
    "def remove_minor_words(wordmap,word_count,min_occurance):\n",
    "    words=list(wordmap.keys())\n",
    "    freq=list(wordmap.values())\n",
    "    word_df=pd.DataFrame(words,index=list(wordmap.values()),columns=['words'])\n",
    "    word_df=word_df.sort_index()\n",
    "    word_df['freq']=wc\n",
    "    word_df=word_df.sort_values('freq',ascending=False)\n",
    "    mask=word_df['freq']<=min_occurance\n",
    "    word_df=word_df[mask]\n",
    "    ss_words=(list(word_df['words']))\n",
    "    print('initial length of wordmap :',len(wordmap))\n",
    "    for word in ss_words:\n",
    "        del wordmap[word]\n",
    "    _wordmap={}\n",
    "    ind=0\n",
    "    for word in wordmap.keys():\n",
    "        _wordmap[word]=ind\n",
    "        ind+=1\n",
    "    print('reduced length of wordmap :',len(_wordmap))\n",
    "    return _wordmap\n",
    "\n",
    "def stem_word(word):\n",
    "    temp=word\n",
    "    word=stem_regular_verb(word)\n",
    "    word=stem_irregular_verb(word)\n",
    "    #word=ps.stem(word)\n",
    "    if(len(word)<=1):\n",
    "        return temp\n",
    "    return word   \n",
    "def remove_tags(doc):\n",
    "    text=doc.lower()\n",
    "    #text=re.sub('\\n',' ',text)\n",
    "    #text=re.sub('-',' ',text)\n",
    "    #text=re.sub('[()?.@*#&!,$]','',text)\n",
    "    #text=re.sub('[\\']','',text)\n",
    "    #text=re.sub('[^a-z0-9]',' ',text)\n",
    "    #words=text.split(' ')\n",
    "    text=re.sub('<[^<>]+>',' ',text)\n",
    "    #text=re.sub('^\\w+',' ',text)\n",
    "    #words=text.split('\\n')\n",
    "    words=re.findall('\\w+',text)\n",
    "    return words\n",
    "def pre_process(topics,max_doc,root):\n",
    "    index=0\n",
    "    labels=[]\n",
    "    ln=0\n",
    "    labeled_docs={}\n",
    "    for topic in topics:\n",
    "        print(topic)\n",
    "        mydoc = minidom.parse(root+topic+'.xml')\n",
    "        documents = mydoc.getElementsByTagName('row')\n",
    "        doc_count=0\n",
    "        docs=[]\n",
    "        for doc in documents:\n",
    "            doc=doc.attributes['Body'].value\n",
    "            if len(doc)<5:\n",
    "                continue\n",
    "            words = remove_tags(doc)\n",
    "            ln+=len(words)\n",
    "            docs.append(words)\n",
    "            labels.append(topic)\n",
    "            doc_count+=1\n",
    "            if doc_count>=max_doc:\n",
    "                break\n",
    "        labeled_docs[topic]=docs\n",
    "    return labeled_docs,ln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3d_Printer\n",
      "Coffee\n",
      "Chess\n",
      "3d_Printer\n",
      "Coffee\n",
      "Chess\n",
      "9983\n"
     ]
    }
   ],
   "source": [
    "root='Dataset/reuters/Training/'\n",
    "root_test='Dataset/Test/'\n",
    "\n",
    "f=open('Dataset/Stopwords.txt')\n",
    "s_words=f.read()\n",
    "s_words=s_words.split('\\n')\n",
    "s_words.append('')\n",
    "\n",
    "f=open('Dataset/verbs.txt','r')\n",
    "verbs=f.read()\n",
    "f.close()\n",
    "verbs=verbs.split('\\n')\n",
    "\n",
    "irr=pd.read_csv('Dataset/irr_verbs.txt',delimiter=',',names=['root','past','past_par'])\n",
    "verb_root=list(irr['root'])\n",
    "past=list(irr['past'])\n",
    "past_pp=list(irr['past_par'])\n",
    "\n",
    "#topics=['3d_Printer','Coffee','Arduino','Astronomy','Biology']#,'Chess','Cooking','Law','Space','Windows_Phone','Wood_Working']\n",
    "topics1=['3d_Printer','Anime','Arduino']#----------Tes topic set 1\n",
    "topics2=['3d_Printer','Coffee','Chess']#-----------Topic set 2\n",
    "topics3=['3d_Printer','Coffee','Chess','Astronomy','Law']#----------Topic set 3\n",
    "#topics=['Arduino','Anime','Windows_Phone']\n",
    "max_docs_train=600\n",
    "max_docs_test=20\n",
    "rm_minor_words=False\n",
    "log=False\n",
    "topics=topics2\n",
    "root='Dataset/Training/'\n",
    "train_docs,ln=pre_process(topics,max_docs_train,root)\n",
    "test_docs,ln=pre_process(topics,max_docs_test,root_test)\n",
    "print(ln)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word to Vector\n",
    "1. Creating dictionary of words from dataset\n",
    "    \n",
    "       Function make dictionary\n",
    "    \n",
    "2. Creating Vector for every class i.e\n",
    "    \n",
    "       Function Vectorize sequence\n",
    "    \n",
    "    if Sample -- > A\n",
    "    \n",
    "              Vector A -- > prob_of_word1,prob_of_word2, prob_of_word3 ...... prob_of_wordn\n",
    "    \n",
    "    if Sample -- > B\n",
    "       \n",
    "              Vector B -- > prob_of_word1,prob_of_word2, prob_of_word3 ...... prob_of_wordn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dictionary(topics,max_doc,all_docs):\n",
    "    index=0\n",
    "    wordmap={}\n",
    "    word_count=[]\n",
    "    for topic in topics:\n",
    "        print(topic)\n",
    "        doc_count=0\n",
    "        docs=all_docs[topic]\n",
    "        for doc in docs:\n",
    "            for word in doc:\n",
    "                if word in s_words or len(word)<=1:\n",
    "                    continue\n",
    "                word=stem_word(word)\n",
    "                if word not in wordmap:\n",
    "                    wordmap[word]=index\n",
    "                    word_count.append(int(1))\n",
    "                    index=index+1\n",
    "                elif word in wordmap:\n",
    "                    ind=wordmap[word]\n",
    "                    word_count[ind]=int(word_count[ind])+1\n",
    "    return wordmap,word_count\n",
    "def vectorize_seq(topics,max_doc,wordmap,all_docs):\n",
    "    n_row=len(topics)*(max_doc)\n",
    "    vectors=np.zeros((n_row,len(wordmap)),dtype=np.float64)\n",
    "    vct_label=[]\n",
    "    row=0\n",
    "    for topic in topics:\n",
    "        print(topic)\n",
    "        doc_count=0\n",
    "        docs=all_docs[topic]\n",
    "        for doc in docs:\n",
    "            #if len(doc)<5:\n",
    "            #    continue\n",
    "            for word in doc:\n",
    "                word=stem_word(word)\n",
    "                if word in wordmap:\n",
    "                    ind=wordmap[word]\n",
    "                    vectors[row,ind]=vectors[row,ind]+1\n",
    "            row=row+1\n",
    "            vct_label.append(topic)\n",
    "            word_list.append(doc)\n",
    "    return vectors,vct_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eucl_dis(test_vect,vectors,vct_labels):\n",
    "    h_dists=[]\n",
    "    for row in range(len(vct_labels)):\n",
    "        vect=vectors[row]\n",
    "        dist=np.abs(vect-test_vect)\n",
    "        dist=np.square(dist)\n",
    "        dist=np.sum(dist)\n",
    "        dist=np.sqrt(dist)\n",
    "        label=vct_labels[row]\n",
    "        h_dists.append([dist,label])\n",
    "    return h_dists\n",
    "def humm_dis(test_vector,vectors,vct_labels):\n",
    "    h_dists=[]\n",
    "    \n",
    "    test_vect=np.array(test_vector>0,dtype=np.float64)\n",
    "    vects=np.array(vectors>0,dtype=np.float64)\n",
    "    #test_vect=test_vector\n",
    "    #vects=vectors\n",
    "    for row in range(len(vct_labels)):\n",
    "        vect=vects[row]\n",
    "        label=vct_labels[row]\n",
    "        dist=np.abs(vect-test_vect)\n",
    "        #dist=dist*[test_vect!=0]\n",
    "        dist=np.sum(dist)\n",
    "        h_dists.append([dist,label])\n",
    "    return h_dists\n",
    "def cos_dis(test_vect,vectors,vct_labels):\n",
    "    h_dists=[]\n",
    "    \n",
    "    vectors_bin= np.array(vectors>0,dtype=np.float64)\n",
    "    wc_vect=np.sum(vectors_bin,axis=0)\n",
    "    no_docs=len(vectors)\n",
    "    \n",
    "    IDF=no_docs/(wc_vect+1)\n",
    "    IDF=np.log(IDF)\n",
    "    \n",
    "    for row in range(len(vct_labels)):\n",
    "        train_vect=vectors[row]\n",
    "        \n",
    "        train_label=vct_labels[row]\n",
    "        \n",
    "        train_total=np.sum(train_vect)\n",
    "        test_total=np.sum(test_vect)\n",
    "        \n",
    "        TF_test=test_vect/test_total\n",
    "        test_vec=TF_test*IDF\n",
    "        \n",
    "        temp=np.square(test_vec)\n",
    "        temp=np.sum(temp)\n",
    "        test_vec_mag=np.sqrt(temp)\n",
    "        \n",
    "        TF_train=train_vect/train_total\n",
    "        train_vec=TF_train*IDF\n",
    "        \n",
    "        temp=np.square(train_vec)\n",
    "        temp=np.sum(temp)\n",
    "        train_vec_mag=np.sqrt(temp)\n",
    "        \n",
    "        X=np.sum(test_vec*train_vec)\n",
    "        Y=train_vec_mag*test_vec_mag\n",
    "        \n",
    "        dist=X/Y\n",
    "        h_dists.append([dist,train_label])\n",
    "    return h_dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(nn,test_vect,vectors,vct_labels,method):\n",
    "    if method=='euc':\n",
    "        h_dists=eucl_dis(test_vect,vectors,vct_labels)\n",
    "    if method=='hum':\n",
    "        h_dists=humm_dis(test_vect,vectors,vct_labels)\n",
    "    if method=='cos':\n",
    "        h_dists=cos_dis(test_vect,vectors,vct_labels)\n",
    "    h_df=pd.DataFrame(h_dists,columns=['dist','label'])\n",
    "    \n",
    "    if method=='euc' or method=='hum':\n",
    "        h_df=h_df.sort_values('dist',ascending=True)\n",
    "    if method=='cos':\n",
    "        h_df=h_df.sort_values('dist',ascending=False)\n",
    "    #print(h_df.head(5))\n",
    "    lb=h_df['label']\n",
    "    lb=lb[:nn]\n",
    "    #print(lb.values)\n",
    "    \n",
    "    if k==1:\n",
    "        lb=lb.mode()\n",
    "        lb=lb.values[0]\n",
    "        \n",
    "    if k==3:\n",
    "        if len(lb.unique())==k:\n",
    "            lb=lb[:1]\n",
    "        lb=lb.mode()\n",
    "        lb=lb.values[0]\n",
    "    if k==5:\n",
    "        if len(lb.mode())>=2:\n",
    "            lb=lb[:3]\n",
    "            if len(lb.mode())>=2:\n",
    "                lb=lb[:1]\n",
    "        lb=lb.mode()\n",
    "        lb=lb.values[0]\n",
    "        \n",
    "                        \n",
    "    return lb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making dictionary :\n",
      "3d_Printer\n",
      "Coffee\n",
      "Chess\n",
      "Vectorizing Sequence:\n",
      "3d_Printer\n",
      "Coffee\n",
      "Chess\n",
      "11904\n"
     ]
    }
   ],
   "source": [
    "word_list=[]\n",
    "min_occurance=1\n",
    "print('Making dictionary :')\n",
    "wordmap,wc=make_dictionary(topics,max_docs_train,train_docs)\n",
    "if rm_minor_words==True:\n",
    "    print('Removing minor words:')\n",
    "    wordmap=remove_minor_words(wordmap,wc,min_occurance)\n",
    "print('Vectorizing Sequence:')\n",
    "vectors,vct_labels=vectorize_seq(topics,max_docs_train,wordmap,train_docs)\n",
    "print(len(wordmap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3d_Printer\n",
      "Coffee\n",
      "Chess\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:17<00:00,  3.62it/s]\n",
      "100%|██████████| 60/60 [00:03<00:00, 18.71it/s]\n",
      "100%|██████████| 60/60 [00:08<00:00,  7.15it/s]\n",
      "100%|██████████| 60/60 [00:17<00:00,  3.68it/s]\n",
      "100%|██████████| 60/60 [00:03<00:00, 19.31it/s]\n",
      "100%|██████████| 60/60 [00:08<00:00,  7.31it/s]\n",
      "100%|██████████| 60/60 [00:16<00:00,  3.67it/s]\n",
      "100%|██████████| 60/60 [00:03<00:00, 19.05it/s]\n",
      "100%|██████████| 60/60 [00:08<00:00,  6.56it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "test_vector,true_lbs=vectorize_seq(topics,max_docs_test,wordmap,test_docs)\n",
    "n=len(topics)*max_docs_test\n",
    "methods=['cos','euc','hum']\n",
    "k_val=[1,3,5]\n",
    "#k_val=[5]\n",
    "no_topics=len(topics)\n",
    "tot_doc=no_topics*max_docs_train\n",
    "tot_doc_test=no_topics*max_docs_test\n",
    "\n",
    "acc_log=''\n",
    "acc_log+='********************************************************************************************\\n'\n",
    "acc_log+=str(topics)+'\\n'\n",
    "acc_log+='stem : none , re : findall , minor_remove: 1\\n'\n",
    "acc_log+=str(['no_topic','train doc per topic','train_doc','test doc per topic','test_doc','word map'])+'\\n'\n",
    "acc_log+=str([no_topics,max_docs_train,tot_doc,max_docs_test,tot_doc_test,str(len(wordmap))])+'\\n'\n",
    "acc_log+='********************************************************************************************\\n'\n",
    "acc_log+=str(['k','method','accuracy','time'])+'\\n'\n",
    "for k in k_val:\n",
    "    for method in methods:\n",
    "        acc=0\n",
    "        T=time.time()\n",
    "        for i in tqdm(range(len(test_vector))):\n",
    "            true_lb=true_lbs[i]\n",
    "            test_vect=test_vector[i,:]\n",
    "            pred_lb=knn(k,test_vect,vectors,vct_labels,method)\n",
    "            #print(pred_lb,true_lb)\n",
    "            if pred_lb==true_lb:\n",
    "                acc=acc+1\n",
    "                #print(pred_lb,true_lb,acc,i+1)\n",
    "        accuracy=acc/(n)\n",
    "        T=time.time()-T\n",
    "        #print('accuray',accuracy)\n",
    "        #print('Time :',T)\n",
    "        acc_log+=(str([k,method,accuracy,T])+'\\n')\n",
    "\n",
    "acc_log=re.sub('[\\[\\] ]','',acc_log)\n",
    "if log==True:\n",
    "    file=open('Report/backup_exp_mod.txt','a')\n",
    "    file.write(acc_log)\n",
    "    file.close()\n",
    "    pass\n",
    "\n",
    "#68.75% eucl 60 82.92\n",
    "#53.33 humm 60  81.25\n",
    "#87.08 cos 94.17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_vectors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-22f5fd29e2b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0m_result_log\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'true'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'pred'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'right'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'wrong'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mknn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_vect\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvct_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cos'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0m_result_log\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'true'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrue_lbs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_vectors' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "_result_log=pd.DataFrame(data={},columns=['true','pred','right','wrong'])\n",
    "\n",
    "for i in range(len(test_vectors)):\n",
    "        pred,pred_label=knn(k,test_vect,vectors,vct_labels,method='cos')\n",
    "        _result_log.loc[i,'true']=true_lbs[i]\n",
    "        _result_log.loc[i,'pred']=pred_label\n",
    "        \n",
    "        if pred_label==true_lbs[i]:\n",
    "            acc=acc+1\n",
    "            _result_log.loc[i,'right']=1\n",
    "            _result_log.loc[i,'wrong']=0\n",
    "        else :\n",
    "            _result_log.loc[i,'right']=0\n",
    "            _result_log.loc[i,'wrong']=1\n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_result_log' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-bceb597c053c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#plt.plot(result_log[['right','wrong']])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mresult_log\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_result_log\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrac\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Stack_Exchange'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name '_result_log' is not defined"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "#plt.plot(result_log[['right','wrong']])\n",
    "result_log=_result_log.sample(frac=1).reset_index()\n",
    "dataset = 'Stack_Exchange'\n",
    "fig=plt.figure(figsize=(15,5))\n",
    "plt.plot(result_log.loc[:,'true'],'r--',alpha=0.6)\n",
    "plt.plot(result_log.loc[:,'pred'],'bx',alpha=0.6)\n",
    "plt.plot(result_log.loc[:,'right'][result_log.loc[:,'right']==1]+1.1,'go',alpha=0.6)\n",
    "plt.plot(result_log.loc[:,'right'][result_log.loc[:,'right']==0]+2.1,'ro',alpha=0.6)\n",
    "#plt.plot(result_log.loc[:100,'right'],'go',alpha=0.3)\n",
    "plt.legend(['true','pred','right','wrong'])\n",
    "plt.title(dataset+' dataset with naive bayes prediction correctness plot')# \\n right : 1 \\n wrong : 0')\n",
    "plt.savefig(dataset+'_KNN_pred.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total=len(result_log)\n",
    "acc=result_log.right.sum()#,len(result_log)\n",
    "accuracy=acc/total\n",
    "labels=result_log['true'].unique()\n",
    "ldf=pd.DataFrame(data={},index=[' '])\n",
    "precisions={}\n",
    "recalls={}\n",
    "for label in labels:\n",
    "    true_ind=result_log['true']==label\n",
    "    false_ind=result_log['true']!=label\n",
    "    Yes=result_log.loc[true_ind]\n",
    "    No=result_log.loc[false_ind]\n",
    "    tp=Yes.right.sum()\n",
    "    tn=Yes.wrong.sum()\n",
    "    fn=No.right.sum()\n",
    "    fp=No.wrong.sum()\n",
    "    recall=tp/(tp+tn)\n",
    "    precision=tp/(tp+fp)\n",
    "    precisions[label]=precision\n",
    "    recalls[label]=recall\n",
    "    print('----------------------')\n",
    "    print('label',label)\n",
    "    print('precision',precision)\n",
    "    print('recall',recall)\n",
    "    print('----------------------')\n",
    "print('Over All Accuracy : ',accuracy)\n",
    "\n",
    "df=pd.DataFrame({},index=[' '])\n",
    "#fig=plt.figure(figsize=(10,10))\n",
    "for label in result_log['true'].unique():\n",
    "    df['class-'+str(label)+'-precision']=[precisions[label],]\n",
    "    df['calss-'+str(label)+'-recall']=[recalls[label],]\n",
    "df['accuracy']=accuracy \n",
    "df.plot.bar()\n",
    "plt.xlim(-.3,.8)\n",
    "plt.title('test result for ' +dataset+ ' data \\n '+'train '+str(len(vectors))+' samples test '+str(len(result_log))+' samples')\n",
    "plt.xlabel('summary of precision, recall, accuracy for iris dataset')\n",
    "plt.savefig(dataset+'_KNN.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************************\n",
      "'3d_Printer','Coffee','Chess'\n",
      "stem:none,re:findall,minor_remove:1\n",
      "'no_topic','traindocpertopic','train_doc','testdocpertopic','test_doc','wordmap'\n",
      "3,600,1800,20,60,'11904'\n",
      "********************************************************************************************\n",
      "'k','method','accuracy','time'\n",
      "1,'cos',0.9333333333333333,17.120943069458008\n",
      "1,'euc',0.8,3.2130320072174072\n",
      "1,'hum',0.85,8.451797008514404\n",
      "3,'cos',0.9666666666666667,17.041046857833862\n",
      "3,'euc',0.8666666666666667,3.1150641441345215\n",
      "3,'hum',0.8166666666666667,8.115386009216309\n",
      "5,'cos',0.9833333333333333,16.433571815490723\n",
      "5,'euc',0.8666666666666667,3.1284570693969727\n",
      "5,'hum',0.8166666666666667,8.269553184509277\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(acc_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Debugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st='i am hasib 3dprinter 4kjl <p> </pl>'\n",
    "st=re.sub('<[^<>]+>',' ',st)\n",
    "s=re.findall('\\w+',st)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open('Report/backup_exp_mod.txt','a')\n",
    "file.write(acc_log)\n",
    "file.close()\n",
    "print(acc_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw='I am hasib\\n,i \"am\" a student() so? @i do& my-duties! porp. and' \n",
    "#text=re.sub('\\n',' ',raw)\n",
    "text=raw.lower()\n",
    "text=re.sub('[^a-z0-9]',' ',text)\n",
    "#text=re.sub('-',' ',text)\n",
    "#text=re.sub('[\\'\"]','',text)\n",
    "#text=re.sub('[()?.@*#&!,$+]','',text)\n",
    "#print(text.split(' '))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
