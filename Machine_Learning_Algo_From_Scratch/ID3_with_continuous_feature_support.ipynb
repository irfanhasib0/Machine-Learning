{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ID3 Implemented from scratch with python\n",
    "** some basic pandas and nupmpy utility functions are used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions\n",
    "1. get_possible_labels\n",
    "2. get_target_label_count\n",
    "3. get_most_common\n",
    "4. get_df_with_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "import csv\n",
    "sys.__stdout__=sys.stdout\n",
    "\n",
    "def get_attr_names(df):\n",
    "    return list(df.columns.values)\n",
    "\n",
    "def get_possible_labels(examples,attr_name):     #getting possible labels of a attribute\n",
    "    attr_column=examples[attr_name]\n",
    "    labels=[]\n",
    "    for m in attr_column:\n",
    "        if m not in labels:\n",
    "            labels.append(m)\n",
    "    return(labels)\n",
    "    #return examples[attr_name].unique()\n",
    "\n",
    "def get_target_label_counts(target_attr,target_labels): #getting target label counts of a dataset\n",
    "    counts={}\n",
    "    for label in target_labels:\n",
    "        counts[label]=0\n",
    "    for j in target_attr.index:\n",
    "            target_val=target_attr[j]\n",
    "            for label in target_labels:\n",
    "                if target_val==label:\n",
    "                    counts[label]=counts[label]+1\n",
    "    return counts\n",
    "    #return dict(target_attr.value_counts())\n",
    "    \n",
    "def get_most_common(counts):\n",
    "    most_common=''\n",
    "    max_count=0\n",
    "    for kk,vv in counts.items():\n",
    "        if int(vv)>max_count:\n",
    "               most_common=kk\n",
    "               max_count=vv\n",
    "    return most_common\n",
    "    \n",
    "def get_df_with_label(examples,target_attr,attr_name,label):                        \n",
    "    attr_attr=examples.loc[:,attr_name]                                  \n",
    "    ret=[]\n",
    "    for index in attr_attr.index:\n",
    "        if attr_attr[index]!=label:\n",
    "            ret.append(index)\n",
    "    examplesVi=examples.drop(ret,axis=0)\n",
    "    target_attrVi=target_attr.drop(ret)\n",
    "    return examplesVi,target_attrVi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data discretization block\n",
    "1. get_thersholds\n",
    "2. get_best_Split\n",
    "3. relabel attributes\n",
    "4. Discretize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_thrsholds(df,attr_name,target_attr):\n",
    "        df[target_name]=target_attr\n",
    "        df_1=df.sort_values(attr_name,axis=0)               #Sorting value\n",
    "        target_attr_1=df_1[target_name]\n",
    "        prev_value=target_attr_1[target_attr_1.index.values[0]] #saving first value as previous value\n",
    "        thrs=[]                                                 #initializing variables\n",
    "        thrs_val=[]\n",
    "        summ=[]\n",
    "        i=0\n",
    "        #print(df_1)\n",
    "        for index in (target_attr_1.index):\n",
    "            thrs.append(index)                                        #saving values until any change in labels\n",
    "            if target_attr_1.loc[index]!=prev_value and len(thrs)>=2: #Comparing value with previous value\n",
    "                summ.append(thrs[:-1])                                #Saving the first group\n",
    "                temp=thrs[-1]\n",
    "                thrs=[]                                                 \n",
    "                thrs.append(temp)\n",
    "                val=df.loc[index,attr_name]                           #getting value of the threshold point \n",
    "                if val not in(thrs_val):                              #checking duplicate\n",
    "                    thrs_val.append(val)                                \n",
    "                \n",
    "                prev_value=target_attr_1.loc[index]                   #\n",
    "        summ.append(thrs)\n",
    "        return(summ,thrs_val)                                       #returning threshold values and index\n",
    "                \n",
    "        #df get_splitted_gain\n",
    "\n",
    "\n",
    "def best_split(df,attr_name,target_attr,thrs_val):\n",
    "        df_left=df\n",
    "        df_right=df\n",
    "        wg_ent_sum=0\n",
    "        total_items=(len(df))\n",
    "        max_gain=0\n",
    "        best_thr=0\n",
    "        tg_label_count=get_target_label_counts(target_attr,target_labels)   #getting target labels counts\n",
    "        ent_s=entropy(tg_label_count)\n",
    "        print(len(thrs_val))\n",
    "        if len(thrs_val)>200:\n",
    "            thrs_val[0]=np.mean(thrs_val)\n",
    "            thrs_val=thrs_val[:1]\n",
    "        for val in thrs_val:                                                ####iteration over each threshold value\n",
    "            df_left=df\n",
    "            df_right=df\n",
    "            #val=thrs_val[0]\n",
    "            #for index,row in df.iterrows():                                 ####spliting for a threshold value\n",
    "            filt_left=df_left[attr_name]>=val\n",
    "            filt_right=df_right[attr_name]<val\n",
    "            df_left=df_left[filt_left]\n",
    "            df_right=df_right[filt_right]\n",
    "            df_list=[df_left,df_right]\n",
    "            wg_ent_sum=0\n",
    "            for sub_df in df_list:                                          #ierating over splited df\n",
    "                target_attrVi=sub_df[target_name]\n",
    "                tg_label_count=get_target_label_counts(target_attrVi,target_labels)\n",
    "                ent=entropy(tg_label_count)\n",
    "                no_items=len(target_attrVi)\n",
    "                wg_ent_sum+=(no_items/total_items)*ent                      ####Getting weighted sum for splited data\n",
    "            #print(ent_s,wg_ent_sum)\n",
    "            gain=ent_s-wg_ent_sum                                           ####Getting gain\n",
    "            if gain>max_gain:\n",
    "                max_gain=gain\n",
    "                best_thr=val                                                ####Getting thresh value with max gain\n",
    "            #print(gain,val,best_thr)\n",
    "        return best_thr\n",
    "        \n",
    "def relabel_attr(df,attr_name,best_thr):                                    \n",
    "    new_df=[]\n",
    "    for index,row in df.iterrows():\n",
    "        #print(df.loc[index,attr_name])\n",
    "        if (df.loc[index,attr_name]>best_thr):                          #Labeling values greater than threshold with descriptive string\n",
    "                new_df.append(attr_name+'>='+str(best_thr))\n",
    "        if (df.loc[index,attr_name]<=best_thr):                         #Labeling values less than threshold with descriptive string\n",
    "                new_df.append(attr_name+'<'+str(best_thr))\n",
    "    return new_df\n",
    "\n",
    "def discretize(df,attr_names,target_attr):\n",
    "    new_dict={}\n",
    "    max_label=25\n",
    "    for attr_name in attr_names:\n",
    "        lb=get_possible_labels(df,attr_name)\n",
    "        if(len(lb)>max_label) and df.dtypes[attr_name] !='object':\n",
    "            print(attr_name)\n",
    "            summ,thrs_val=get_thrsholds(df,attr_name,target_attr)\n",
    "            best_thr=best_split(df,attr_name,target_attr,thrs_val)\n",
    "            new_attr=relabel_attr(df,attr_name,best_thr)\n",
    "            new_dict[attr_name]=new_attr\n",
    "        else:\n",
    "            labels=get_possible_labels(df,attr_name)\n",
    "            lb_num=len(labels)\n",
    "            if lb_num<=max_label:\n",
    "                new_dict[attr_name]=list(df[attr_name])\n",
    "    #new_dict[target_name]=list(df.loc[:,target_name])\n",
    "    new_df=pd.DataFrame(new_dict)\n",
    "    return(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ID3 Funtions\n",
    "1. Entropy\n",
    "2. Gain\n",
    "3. ID3\n",
    "4. build tree\n",
    "5. predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(counts):\n",
    "    total=0\n",
    "    ent=0\n",
    "    counts=list(counts.values())\n",
    "    total=np.sum(counts)\n",
    "    if total==0:\n",
    "        return ent\n",
    "    for val in counts:\n",
    "        p=val/total\n",
    "        if p!=0:\n",
    "            ent-=1*p*np.log2(p)\n",
    "    return ent\n",
    "\n",
    "def gain(examples,attr_name,target_attr,target_labels):\n",
    "\n",
    "    tg_label_count=get_target_label_counts(target_attr,target_labels)            #Getting label(+/-/others) count for dataset/subset\n",
    "    ent_s=entropy(tg_label_count)                                                #Getting entropy for the dataset/subset\n",
    "    total_items=len(examples)                                                      #getting total items count in attrumn(attribute)\n",
    "    gain=0\n",
    "    split_info=0\n",
    "    labels=get_possible_labels(examples,attr_name)                                  #Getting possible labels for the attribute\n",
    "    for i in range(len(labels)):\n",
    "        _,target_attrVi=get_df_with_label(examples,target_attr,attr_name,labels[i]) #extract subset with label given\n",
    "        tg_label_count=get_target_label_counts(target_attrVi,target_labels)         #Getting label(+/-/others) count for dataset/subset                                                                    #Running iteration over splitted labels\n",
    "\n",
    "        ent=entropy(tg_label_count)                                                 #Getting entropy of each split\n",
    "        no_of_items=len(target_attrVi)                                                 #weighted sum of entropies of splits\n",
    "        ratio=no_of_items/total_items\n",
    "        gain=gain+ent*(ratio)\n",
    "\n",
    "        \n",
    "        if ratio!=0:\n",
    "            split_info-=(ratio*np.log2(ratio))                           #Getting weighted sum of split info\n",
    "            \n",
    "    gain=ent_s-gain                                                                #Getting information gain               \n",
    "    if split_info!=0:\n",
    "        gain_ratio=gain/split_info                                          \n",
    "    #print('Log ....',attr_name,ent_s,gain)\n",
    "    return split_info,gain\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def ID3(examples,target_attr,attr_names):\n",
    "    global summary,res\n",
    "    tg_label_count=get_target_label_counts(target_attr,target_labels)              ###Getting target label counts\n",
    "    total=len(target_attr)\n",
    "    for label,count in tg_label_count.items():                                     ###Check purity=1 for all target label\n",
    "        if count==total:\n",
    "           return label\n",
    "    most_common=get_most_common(tg_label_count)\n",
    "    if len(attr_names)==0:                                                          ###Check for empty dataset                  \n",
    "            return most_common\n",
    "    max_gain=-1                                                                     ###Getting attribute with highest gain\n",
    "    attr_name=''\n",
    "    for i in range(len(attr_names)):                                                 \n",
    "        _,gain_value= gain(examples,attr_names[i],target_attr,target_labels)\n",
    "        if gain_value>max_gain:\n",
    "            max_gain=gain_value\n",
    "            attr_name=attr_names[i]\n",
    "    print('attr_name',attr_name)\n",
    "    labels=get_possible_labels(examples,attr_name)                                      ###Getting possible labels for bes attribute\n",
    "    print('labels',labels)\n",
    "    for label in labels:\n",
    "        examplesVi,target_attrVi=get_df_with_label(examples,target_attr,attr_name,label)###Getting new subset of dataset\n",
    "        examplesVi=examplesVi.drop(attr_name,axis=1)                            \n",
    "        res.append(attr_name)                                                           ###returning attr_name,label pair\n",
    "        res.append(label)\n",
    "        attr_names=get_attr_names(examplesVi)\n",
    "        if len(attr_names)==0:# len(target_attrVi)>=2:                                    ###Checking for empty datasets                                                                                         #Getting most common label\n",
    "            return most_common\n",
    "        else:\n",
    "            ret=ID3(examplesVi,target_attrVi,attr_names)                                ###Calling ID3\n",
    "            if ret !=None:\n",
    "                res.append(target_name)\n",
    "                res.append(ret)\n",
    "                summary.append(res)\n",
    "                res=[]\n",
    "    return \n",
    "\n",
    "\n",
    "#print('ret',summary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_tree(summary):\n",
    "    root=summary[0][0]\n",
    "    for k in range(len(summary)):\n",
    "        if summary[k][0]==root:\n",
    "            temp=summary[k]\n",
    "        if summary[k][0]!=root:\n",
    "            #print(temp)\n",
    "            for t in range(len(temp)):\n",
    "                if temp[t]==summary[k][0]:\n",
    "                    temp=temp[:t]\n",
    "                    break\n",
    "            summary[k]=temp+summary[k]\n",
    "        print(summary[k])\n",
    "\n",
    "def predict(test_df,test_target,summary):\n",
    "    acc=0\n",
    "    for kk in range(len(test_df)):\n",
    "         test=test_df.loc[kk,:]\n",
    "         #print('..............',kk)\n",
    "         for i in range(len(summary)):\n",
    "            m=len(summary[i])                   #Taking a rule\n",
    "            for j in range(int(m/2)):\n",
    "                attr=summary[i][2*j]            #Taking a attr\n",
    "                lb=summary[i][2*j+1]            #Taking corresponding label\n",
    "                #print('attr',attr,'lb',lb)\n",
    "                if attr!=target_name:           #last node is not reached\n",
    "                   if test[attr] != lb:         #Checking test data having same label \n",
    "                        break                   #if not break and go for next rule\n",
    "            if attr == target_name:             #if test data satisfy have all the attr>> label in rule\n",
    "                pred_label=lb                          #setting result according to rule\n",
    "                true_label=test_target[kk]\n",
    "                #print(kk,'rule',i,test_target[kk],'res',res)\n",
    "                if true_label==pred_label:        #Checking results validity from true label\n",
    "                    acc=acc+1\n",
    "                    tp[pred_label]+=1\n",
    "                #break\n",
    "                if true_label!=pred_label:\n",
    "                     tn[pred_label]+=1\n",
    "                     fp[true_label]+=1\n",
    "                break                           \n",
    "            if attr==target_name:\n",
    "                break\n",
    "    accuracy=acc/len(test_df)*100\n",
    "    \n",
    "    for label in target_labels:\n",
    "            try:recall[label]=tp[label]/(tp[label]+tn[label])\n",
    "            except:0\n",
    "            try:precision[label]=tp[label]/(tp[label]+fp[label])\n",
    "            except:0\n",
    "            print(label,'precision',precision[label],'recall',recall[label])\n",
    "    \n",
    "    print('Summary : ','correct:',acc, 'total :',len(test_df),'accuracy :',acc/len(test_df)*100,'%')\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sepal_L', 'Sepal_W', 'Petal_L', 'Petal_W'] ['setosa', 'versicolor', 'virginica']\n",
      "                      Sepal_L       Sepal_W       Petal_L       Petal_W\n",
      "0   (4.2989999999999995, 5.1]    (3.3, 4.4]  (0.999, 1.6]  (0.099, 0.3]\n",
      "1   (4.2989999999999995, 5.1]    (2.8, 3.0]  (0.999, 1.6]  (0.099, 0.3]\n",
      "2   (4.2989999999999995, 5.1]    (3.0, 3.3]  (0.999, 1.6]  (0.099, 0.3]\n",
      "3   (4.2989999999999995, 5.1]    (3.0, 3.3]  (0.999, 1.6]  (0.099, 0.3]\n",
      "4   (4.2989999999999995, 5.1]    (3.3, 4.4]  (0.999, 1.6]  (0.099, 0.3]\n",
      "..                        ...           ...           ...           ...\n",
      "95                 (5.1, 5.8]    (2.8, 3.0]   (1.6, 4.35]    (0.3, 1.3]\n",
      "96                 (5.1, 5.8]    (2.8, 3.0]   (1.6, 4.35]    (0.3, 1.3]\n",
      "97                 (5.8, 6.4]    (2.8, 3.0]   (1.6, 4.35]    (0.3, 1.3]\n",
      "98  (4.2989999999999995, 5.1]  (1.999, 2.8]   (1.6, 4.35]    (0.3, 1.3]\n",
      "99                 (5.1, 5.8]  (1.999, 2.8]   (1.6, 4.35]    (0.3, 1.3]\n",
      "\n",
      "[100 rows x 4 columns]\n",
      "Press Enter to continue:\n",
      "attr_name Petal_L\n",
      "labels [Interval(0.999, 1.6, closed='right'), Interval(1.6, 4.35, closed='right'), Interval(4.35, 5.1, closed='right'), Interval(5.1, 6.9, closed='right')]\n",
      "attr_name Sepal_W\n",
      "labels [Interval(3.3, 4.4, closed='right'), Interval(3.0, 3.3, closed='right'), Interval(1.999, 2.8, closed='right'), Interval(2.8, 3.0, closed='right')]\n",
      "attr_name Petal_W\n",
      "labels [Interval(1.3, 1.8, closed='right'), Interval(0.3, 1.3, closed='right'), Interval(1.8, 2.5, closed='right')]\n",
      "attr_name Sepal_L\n",
      "labels [Interval(6.4, 7.9, closed='right'), Interval(5.8, 6.4, closed='right'), Interval(5.1, 5.8, closed='right'), Interval(4.2989999999999995, 5.1, closed='right')]\n",
      "attr_name Sepal_W\n",
      "labels [Interval(3.0, 3.3, closed='right'), Interval(2.8, 3.0, closed='right'), Interval(1.999, 2.8, closed='right'), Interval(3.3, 4.4, closed='right')]\n",
      "['Petal_L', Interval(0.999, 1.6, closed='right'), 'Species', 'setosa']\n",
      "['Petal_L', Interval(1.6, 4.35, closed='right'), 'Sepal_W', Interval(3.3, 4.4, closed='right'), 'Species', 'setosa']\n",
      "['Petal_L', Interval(1.6, 4.35, closed='right'), 'Sepal_W', Interval(3.0, 3.3, closed='right'), 'Species', 'setosa']\n",
      "['Petal_L', Interval(1.6, 4.35, closed='right'), 'Sepal_W', Interval(1.999, 2.8, closed='right'), 'Species', 'versicolor']\n",
      "['Petal_L', Interval(1.6, 4.35, closed='right'), 'Sepal_W', Interval(2.8, 3.0, closed='right'), 'Species', 'versicolor']\n",
      "['Petal_L', Interval(4.35, 5.1, closed='right'), 'Petal_W', Interval(1.3, 1.8, closed='right'), 'Sepal_L', Interval(6.4, 7.9, closed='right'), 'Species', 'versicolor']\n",
      "['Petal_L', Interval(4.35, 5.1, closed='right'), 'Petal_W', Interval(1.3, 1.8, closed='right'), 'Sepal_L', Interval(5.8, 6.4, closed='right'), 'Sepal_W', Interval(3.0, 3.3, closed='right'), 'Species', 'versicolor']\n",
      "['Petal_L', Interval(4.35, 5.1, closed='right'), 'Petal_W', Interval(1.3, 1.8, closed='right'), 'Sepal_L', Interval(5.1, 5.8, closed='right'), 'Species', 'versicolor']\n",
      "['Petal_L', Interval(4.35, 5.1, closed='right'), 'Petal_W', Interval(1.3, 1.8, closed='right'), 'Sepal_L', Interval(4.2989999999999995, 5.1, closed='right'), 'Species', 'virginica']\n",
      "['Petal_L', Interval(4.35, 5.1, closed='right'), 'Petal_W', Interval(0.3, 1.3, closed='right'), 'Species', 'versicolor']\n",
      "['Petal_L', Interval(4.35, 5.1, closed='right'), 'Petal_W', Interval(1.8, 2.5, closed='right'), 'Species', 'virginica']\n",
      "['Petal_L', Interval(5.1, 6.9, closed='right'), 'Species', 'virginica']\n",
      "setosa precision 0 recall 0\n",
      "versicolor precision 0 recall 0\n",
      "virginica precision 1.0 recall 1.0\n",
      "Summary :  correct: 24 total : 30 accuracy : 80.0 %\n",
      "12\n",
      "Press Enter for reduced error pruning\n",
      "['Petal_L', Interval(0.999, 1.6, closed='right'), 'Species', 'setosa']\n",
      "setosa precision 0 recall 0\n",
      "versicolor precision 0 recall 0\n",
      "virginica precision 1.0 recall 1.0\n",
      "Summary :  correct: 24 total : 30 accuracy : 80.0 %\n",
      "80.0\n",
      "['Petal_L', Interval(1.6, 4.35, closed='right'), 'Species', 'setosa']\n",
      "setosa precision 0 recall 0\n",
      "versicolor precision 0 recall 0\n",
      "virginica precision 1.0 recall 1.0\n",
      "Summary :  correct: 24 total : 30 accuracy : 80.0 %\n",
      "80.0\n",
      "['Petal_L', Interval(1.6, 4.35, closed='right'), 'Species', 'setosa']\n",
      "setosa precision 0 recall 0\n",
      "versicolor precision 0 recall 0\n",
      "virginica precision 1.0 recall 1.0\n",
      "Summary :  correct: 24 total : 30 accuracy : 80.0 %\n",
      "80.0\n",
      "['Petal_L', Interval(1.6, 4.35, closed='right'), 'Species', 'versicolor']\n",
      "setosa precision 0 recall 0\n",
      "versicolor precision 0 recall 0\n",
      "virginica precision 1.0 recall 1.0\n",
      "Summary :  correct: 24 total : 30 accuracy : 80.0 %\n",
      "80.0\n",
      "['Petal_L', Interval(1.6, 4.35, closed='right'), 'Species', 'versicolor']\n",
      "setosa precision 0 recall 0\n",
      "versicolor precision 0 recall 0\n",
      "virginica precision 1.0 recall 1.0\n",
      "Summary :  correct: 24 total : 30 accuracy : 80.0 %\n",
      "80.0\n",
      "['Petal_L', Interval(4.35, 5.1, closed='right'), 'Petal_W', Interval(1.3, 1.8, closed='right'), 'Species', 'versicolor']\n",
      "setosa precision 0 recall 0\n",
      "versicolor precision 0 recall 0.0\n",
      "virginica precision 0.9655172413793104 recall 1.0\n",
      "Summary :  correct: 24 total : 30 accuracy : 80.0 %\n",
      "80.0\n",
      "['Petal_L', Interval(4.35, 5.1, closed='right'), 'Petal_W', Interval(1.3, 1.8, closed='right'), 'Sepal_L', Interval(5.8, 6.4, closed='right'), 'Species', 'versicolor']\n",
      "setosa precision 0 recall 0\n",
      "versicolor precision 0 recall 0.0\n",
      "virginica precision 0.9411764705882353 recall 1.0\n",
      "Summary :  correct: 24 total : 30 accuracy : 80.0 %\n",
      "80.0\n",
      "['Petal_L', Interval(4.35, 5.1, closed='right'), 'Petal_W', Interval(1.3, 1.8, closed='right'), 'Species', 'versicolor']\n",
      "setosa precision 0 recall 0\n",
      "versicolor precision 0 recall 0.0\n",
      "virginica precision 0.9230769230769231 recall 1.0\n",
      "Summary :  correct: 24 total : 30 accuracy : 80.0 %\n",
      "80.0\n",
      "['Petal_L', Interval(4.35, 5.1, closed='right'), 'Petal_W', Interval(1.3, 1.8, closed='right'), 'Species', 'virginica']\n",
      "setosa precision 0 recall 0\n",
      "versicolor precision 0 recall 0.0\n",
      "virginica precision 0.9318181818181818 recall 1.0\n",
      "Summary :  correct: 30 total : 30 accuracy : 100.0 %\n",
      "100.0\n",
      "['Petal_L', Interval(4.35, 5.1, closed='right'), 'Species', 'versicolor']\n",
      "setosa precision 0 recall 0\n",
      "versicolor precision 0 recall 0.0\n",
      "virginica precision 0.9047619047619048 recall 1.0\n",
      "Summary :  correct: 20 total : 30 accuracy : 66.66666666666666 %\n",
      "66.66666666666666\n",
      "['Petal_L', Interval(4.35, 5.1, closed='right'), 'Species', 'virginica']\n",
      "setosa precision 0 recall 0\n",
      "versicolor precision 0 recall 0.0\n",
      "virginica precision 0.9135802469135802 recall 1.0\n",
      "Summary :  correct: 30 total : 30 accuracy : 100.0 %\n",
      "100.0\n",
      "['Petal_L', Interval(5.1, 6.9, closed='right'), 'Species', 'virginica']\n",
      "setosa precision 0 recall 0\n",
      "versicolor precision 0 recall 0.0\n",
      "virginica precision 0.9195402298850575 recall 1.0\n",
      "Summary :  correct: 24 total : 30 accuracy : 80.0 %\n",
      "80.0\n",
      "summary [['Petal_L', Interval(0.999, 1.6, closed='right'), 'Species', 'setosa'], ['Petal_L', Interval(1.6, 4.35, closed='right'), 'Sepal_W', Interval(3.3, 4.4, closed='right'), 'Species', 'setosa'], ['Petal_L', Interval(1.6, 4.35, closed='right'), 'Sepal_W', Interval(3.0, 3.3, closed='right'), 'Species', 'setosa'], ['Petal_L', Interval(1.6, 4.35, closed='right'), 'Sepal_W', Interval(1.999, 2.8, closed='right'), 'Species', 'versicolor'], ['Petal_L', Interval(1.6, 4.35, closed='right'), 'Sepal_W', Interval(2.8, 3.0, closed='right'), 'Species', 'versicolor'], ['Petal_L', Interval(4.35, 5.1, closed='right'), 'Petal_W', Interval(1.3, 1.8, closed='right'), 'Sepal_L', Interval(6.4, 7.9, closed='right'), 'Species', 'versicolor'], ['Petal_L', Interval(4.35, 5.1, closed='right'), 'Petal_W', Interval(1.3, 1.8, closed='right'), 'Sepal_L', Interval(5.8, 6.4, closed='right'), 'Sepal_W', Interval(3.0, 3.3, closed='right'), 'Species', 'versicolor'], ['Petal_L', Interval(4.35, 5.1, closed='right'), 'Petal_W', Interval(1.3, 1.8, closed='right'), 'Sepal_L', Interval(5.1, 5.8, closed='right'), 'Species', 'versicolor'], ['Petal_L', Interval(4.35, 5.1, closed='right'), 'Petal_W', Interval(1.3, 1.8, closed='right'), 'Sepal_L', Interval(4.2989999999999995, 5.1, closed='right'), 'Species', 'virginica'], ['Petal_L', Interval(4.35, 5.1, closed='right'), 'Petal_W', Interval(0.3, 1.3, closed='right'), 'Species', 'versicolor'], ['Petal_L', Interval(4.35, 5.1, closed='right'), 'Petal_W', Interval(1.8, 2.5, closed='right'), 'Species', 'virginica'], ['Petal_L', Interval(5.1, 6.9, closed='right'), 'Species', 'virginica']]\n",
      "i  11\n",
      "max_accuracy 100.0\n"
     ]
    }
   ],
   "source": [
    "#names=['A','B','C','D','E','F','G','H','I','J']\n",
    "df=pd.read_csv('Dataset/iris.csv',delimiter=',' )#,names=names)\n",
    "\n",
    "#df=df.sample(frac=1).reset_index(drop=True)\n",
    "#print(df.head(10))\n",
    "#input()\n",
    "df=df.fillna(df.mean())\n",
    "#df=df.replace(np.NaN,0)\n",
    "#df=df.drop('day',axis=1#df=df.drop('Ticket',axis=1)\n",
    "\n",
    "summary=[]\n",
    "res=[]\n",
    "attr_names=get_attr_names(df)\n",
    "target_name=attr_names[-1]\n",
    "target_attr=df[target_name]\n",
    "target_labels=get_possible_labels(df,target_name)\n",
    "\n",
    "tg_label_count=get_target_label_counts(target_attr,target_labels)\n",
    "most_common=get_most_common(tg_label_count)\n",
    "\n",
    "fp={}\n",
    "tn={}\n",
    "tp={}\n",
    "precision={}\n",
    "recall={}\n",
    "for label in target_labels:\n",
    "    fp[label]=0\n",
    "    tn[label]=0\n",
    "    tp[label]=0\n",
    "    recall[label]=0\n",
    "    precision[label]=0\n",
    "\n",
    "df=df.drop(target_name,axis=1)\n",
    "attr_names=attr_names[:-1]\n",
    "#df=discretize(df,attr_names,target_attr)\n",
    "for attr in attr_names:\n",
    "    if df[attr].dtype!=object:\n",
    "        temp=pd.qcut(df[attr],4)\n",
    "        df[attr]=temp\n",
    "attr_names=get_attr_names(df)\n",
    "print(attr_names,target_labels)\n",
    "print(df.head(100))\n",
    "input('Press Enter to continue:')\n",
    "split=int(0.80*len(df))\n",
    "#split=0\n",
    "test_df=df.loc[split:,:]\n",
    "test_target=target_attr.loc[split:]\n",
    "test_df=test_df.reset_index(drop=True)\n",
    "test_target=test_target.reset_index(drop=True)\n",
    "#split=len(df)\n",
    "df=df.loc[:split,:]\n",
    "target_attr=target_attr.loc[:split]\n",
    "\n",
    "#for attr_name in attr_names:\n",
    "#    split_info,gain_value= gain(df,attr_name,target_attr,target_labels)\n",
    "#    print(attr_name,split_info)\n",
    "#    input()\n",
    "\n",
    "ID3(df,target_attr,attr_names)\n",
    "build_tree(summary)\n",
    "accuracy=predict(test_df,test_target,summary)\n",
    "print(len(summary))\n",
    "input('Press Enter for reduced error pruning')\n",
    "accuracy=0\n",
    "max_i=0\n",
    "max_acc=0\n",
    "max_summary=''\n",
    "\n",
    "for i in range(len(summary)):\n",
    "        temp_rule=summary[i]\n",
    "        if (len(summary[i])>4):\n",
    "            temp=summary[i][-2:]\n",
    "            summary[i]=summary[i][:-4]\n",
    "            summary[i]=summary[i]+temp\n",
    "        print(summary[i])\n",
    "        if accuracy>=max_acc:\n",
    "            max_acc=accuracy\n",
    "            max_summary=summary\n",
    "            max_i=i\n",
    "        accuracy=predict(test_df,test_target,summary)\n",
    "        print(accuracy)\n",
    "        summary[i]=temp_rule\n",
    "print('summary',max_summary)\n",
    "print('i ',max_i)\n",
    "print('max_accuracy',max_acc)\n",
    "#i  8\n",
    "#max_accuracy 66.76300578034682\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Petal_L (0.999, 1.6] Species setosa \n",
      "Petal_L (1.6, 4.35] Sepal_W (3.3, 4.4] Species setosa \n",
      "Petal_L (1.6, 4.35] Sepal_W (3.0, 3.3] Species setosa \n",
      "Petal_L (1.6, 4.35] Sepal_W (1.999, 2.8] Species versicolor \n",
      "Petal_L (1.6, 4.35] Sepal_W (2.8, 3.0] Species versicolor \n",
      "Petal_L (4.35, 5.1] Petal_W (1.3, 1.8] Sepal_L (6.4, 7.9] Species versicolor \n",
      "Petal_L (4.35, 5.1] Petal_W (1.3, 1.8] Sepal_L (5.8, 6.4] Sepal_W (3.0, 3.3] Species versicolor \n",
      "Petal_L (4.35, 5.1] Petal_W (1.3, 1.8] Sepal_L (5.1, 5.8] Species versicolor \n",
      "Petal_L (4.35, 5.1] Petal_W (1.3, 1.8] Sepal_L (4.2989999999999995, 5.1] Species virginica \n",
      "Petal_L (4.35, 5.1] Petal_W (0.3, 1.3] Species versicolor \n",
      "Petal_L (4.35, 5.1] Petal_W (1.8, 2.5] Species virginica \n",
      "Petal_L (5.1, 6.9] Species virginica \n"
     ]
    }
   ],
   "source": [
    "for node in summary:\n",
    "    for edge in node:\n",
    "        print(edge,end=' ')\n",
    "    print('')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
